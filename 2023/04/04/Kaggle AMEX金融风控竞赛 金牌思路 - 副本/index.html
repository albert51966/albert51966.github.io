<!DOCTYPE html>
<html lang=zh-CN>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:description" content="卫豪才的博客">
    <meta property="og:type" content="website">
    <meta name="description" content="卫豪才的博客">
    <meta name="keyword"  content="卫豪才,卫豪才的博客,日常分享,技术总结">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        CCF BDCI2023 返乡发展人群预测 第一名思路 - Albert 日常
        
    </title>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/aircloud.css">

    
<link rel="stylesheet" href="/css/gitment.css">

    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_28hi1hpxx24.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>

    









<meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Albert 日常" type="application/atom+xml">
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> sometimes study, sometimes code </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>Haocai Wei</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/collect/">
                    <i class="iconfont icon-shoucang1"></i>
                    <span>收藏</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%99%E5%9C%A8%E5%BC%80%E5%A4%B4"><span class="toc-text">写在开头</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%9B%E9%A2%98%E4%BB%BB%E5%8A%A1"><span class="toc-text">赛题任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-text">评价指标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%8B%E7%BB%8D"><span class="toc-text">数据介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-text">数据分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E9%A2%98%E8%A6%81%E7%82%B9"><span class="toc-text">解题要点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E6%96%B9%E6%B3%95"><span class="toc-text">数据压缩方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%91%E6%A8%A1%E5%9E%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">树模型模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-text">定义模型参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%A4%9A%E6%8A%98%E4%BA%A4%E5%8F%89%E8%AE%AD%E7%BB%83"><span class="toc-text">模型多折交叉训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B"><span class="toc-text">深度学习模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-text">特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#meta-features%EF%BC%88%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%81%EF%BC%89"><span class="toc-text">meta features（最重要的特征！）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E6%B2%A1%E6%9C%89meta-features%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-text">对于没有meta features的模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%E6%96%B9%E6%B3%95%EF%BC%88meta-aggs%EF%BC%89%EF%BC%9A"><span class="toc-text">混合方法（meta+aggs）：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model"><span class="toc-text">Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%E6%8A%80%E5%B7%A7"><span class="toc-text">模型融合技巧</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8%E6%88%96%E6%B2%A1%E6%9C%89%E7%94%A8%EF%BC%9F"><span class="toc-text">什么不起作用或没有用？</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-bg" id="search-bg"></div>
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> sometimes study, sometimes code </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        CCF BDCI2023 返乡发展人群预测 第一名思路
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2023-04-04 09:27:05</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#BDCI" title="BDCI">BDCI</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#AI竞赛" title="AI竞赛">AI竞赛</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#结构化" title="结构化">结构化</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content no-indent">
        <h2 id="写在开头"><a href="#写在开头" class="headerlink" title="写在开头"></a>写在开头</h2><p>本次比赛是中国计算机学会举办的，<strong>第十届大数据与计算智能大赛</strong>下面唯二的两个结构化赛题，最终名次是<strong>（1/2297)<strong>。说实话这道题可供总结和学习的地方不多，数据是标准的</strong>简单结构化</strong>类型，不包含任何时间、多模态（图像、文本、语音）信息需要处理。重点是如何做好数据挖掘工作，包括<strong>除噪</strong>（最重要），数据分布差异检验，无标签数据的认识，训练集、测试集A、测试集B数据特点和差异的挖掘等。赛题数据经过模拟和脱敏后一言难尽，<strong>线上线下不一致</strong>，本地CV不可信，我的方法主要就是靠不断提交去试（不知道大佬怎么去做），我最后提交了166次，最高队伍我看提交了快200次。（奖金还没收到，不敢再往下评价赛题，以后有机会细说），总而言之<strong>新手可以花两分钟看一下思路，不用自己去试，可供参考的意义不大</strong>。</p>
<h2 id="赛题任务"><a href="#赛题任务" class="headerlink" title="赛题任务"></a>赛题任务</h2><p>近年来，中国经济飞速发展，国际地位和国际影响力不断提高，科技发展水平更是突飞猛进。以北京、上海、广州、深圳等城市为首的一线大城市，更是带领着我国经济稳步发展。然而，随着一线城市的生活压力不断增大，越来越多的年轻人选择了返回家乡发展。</p>
<p>在本次比赛中，您将运用<strong>机器学习</strong>技能，基于中国联通的大数据能力，通过使用对联通的<strong>信令数据、通话数据、互联网行为</strong>等数据进行建模，对个人是否会<strong>返乡工作</strong>进行判断。</p>
<p>比赛链接：</p>
<p><a target="_blank" rel="noopener" href="https://www.datafountain.cn/competitions/581">https://www.datafountain.cn/competitions/581</a></p>
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p>本次比赛使用ROC曲线下面积AUC（Area Under Curve）作为评价指标，AUC越大，预测越准确。<img src="/img/bdci_auc.png" alt="img"></p>
<p>其本质是预测结果正样本排在负样本前面的概率，反映的是分类器对样本的排序能力，因此我们需要输出的是概率而不是01分类。</p>
<h2 id="数据介绍"><a href="#数据介绍" class="headerlink" title="数据介绍"></a><strong>数据介绍</strong></h2><p>赛题的目的是基于中国联通的大数据能力，通过使用对联通的信令、通话、互联网行为等数据进行建模，预测样本返回家乡工作发展的概率。 赛题数据由联通基站产生的用户信令数据、联通用户上网产生的上网行为数据及用户日常通话、短信产生的数据，并<strong>通过了匿名和模拟处理</strong>，特征可以分为以下类别：</p>
<p>本次比赛的目的是根据客户每月的客户资料预测客户未来不偿还信用卡余额的概率。目标二元变量是通过观察最近一次信用卡账单后 18 个月的绩效窗口来计算的，如果客户在最近一次账单日后的 120 天内未支付到期金额，则将其视为违约事件。</p>
<ul>
<li>位置类特特征</li>
<li>互联网类特征</li>
<li>通话类特征</li>
</ul>
<p><strong>其中这些变量为类别变量：</strong></p>
<p> [‘B_30’, ‘B_38’, ‘D_114’, ‘D_116’, ‘D_117’, ‘D_120’, ‘D_126’, ‘D_63’, ‘D_64’, ‘D_66’, ‘D_68’]</p>
<p>是为每个customer_ID预测未来付款违约的概率（目标 = 1），每个文件内容的介绍如下：</p>
<p>train_data.csv - 每个 customer_ID 具有多个日期的训练数据</p>
<p>train_labels.csv - 每个 customer_ID 的目标标签</p>
<p>test_data.csv - 对应的测试数据；您的目标是预测每个 customer_ID 的目标标签</p>
<p>sample_submission.csv - 格式正确的示例提交文件</p>
<h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><p>首先我们来查看下数据集，在数据集中的数据字段基本进行了匿名处理：</p>
<p><img src="/img/amex_shujuji1.png" alt="image-20230404005728070"></p>
<p>在数据集中也有较多的字段包含了缺失值：</p>
<p><img src="/img/amex_shujuji2.png" alt="image-20230404005839357"></p>
<p>比赛标签分布中违约用户占比较少，类别分布比较均衡：</p>
<p><img src="/img/amex_shujuji3.png" alt="image-20230404011109833"></p>
<p>接下来我们可以查看字段按照分类之后的统计，如下图所示：</p>
<p><img src="/img/amex_shujuji4.png" alt="image-20230404011141158"></p>
<p>进一步我们也可以统计字段与标签的相关性：</p>
<p><img src="/img/amex_shujuji5.png" alt="image-20230404011208752"></p>
<h2 id="解题要点"><a href="#解题要点" class="headerlink" title="解题要点"></a><strong>解题要点</strong></h2><h4 id="数据压缩方法"><a href="#数据压缩方法" class="headerlink" title="数据压缩方法"></a><strong>数据压缩方法</strong></h4><p>由于数据集原始数据集文件较大（50GB左右），直接进行读取并进行分析并不可取。在进行送入模型进行之前可以考虑对数据值进行压缩，具体的压缩方法包括：</p>
<p>•将数据集中的类别字段，使用Pandas的category进行代替；</p>
<p>•将数据集中的数值字段，使用flaoat16和float32代替原始的float64；</p>
<p>•将数据集使用feather或者parquet格式进行存储；</p>
<p>通过上述方法可以将数据集压缩到总共5GB左右，接下来就可以尝试后续的建模过程。</p>
<p><strong>数据集压缩和转换的过程可以参考：</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/code/abdellatifsassioui/create-pickeld-data-from-50-gb-to-6gb">https://www.kaggle.com/code/abdellatifsassioui/create-pickeld-data-from-50-gb-to-6gb</a></p>
<h4 id="树模型模型"><a href="#树模型模型" class="headerlink" title="树模型模型"></a><strong>树模型模型</strong></h4><p>本次赛题是一个典型的匿名结构化比赛，因此可以考虑直接使用树模型来进行建模，具体的步骤为：</p>
<p>•对数据集进行处理</p>
<p>•定义树模型进行训练</p>
<p>•五折交叉完成预测</p>
<h4 id="定义模型参数"><a href="#定义模型参数" class="headerlink" title="定义模型参数"></a><strong>定义模型参数</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># XGB MODEL PARAMETERS </span></span><br><span class="line">xgb_parms = &#123;      </span><br><span class="line">	<span class="string">&#x27;max_depth&#x27;</span>:<span class="number">4</span>,      </span><br><span class="line">  <span class="string">&#x27;learning_rate&#x27;</span>:<span class="number">0.05</span>,      </span><br><span class="line">  <span class="string">&#x27;subsample&#x27;</span>:<span class="number">0.8</span>,     </span><br><span class="line">  <span class="string">&#x27;colsample_bytree&#x27;</span>:<span class="number">0.6</span>,      </span><br><span class="line">  <span class="string">&#x27;eval_metric&#x27;</span>:<span class="string">&#x27;logloss&#x27;</span>,     </span><br><span class="line">  <span class="string">&#x27;objective&#x27;</span>:<span class="string">&#x27;binary:logistic&#x27;</span>,     </span><br><span class="line">  <span class="string">&#x27;tree_method&#x27;</span>:<span class="string">&#x27;gpu_hist&#x27;</span>,     </span><br><span class="line">  <span class="string">&#x27;predictor&#x27;</span>:<span class="string">&#x27;gpu_predictor&#x27;</span>,     </span><br><span class="line">  <span class="string">&#x27;random_state&#x27;</span>:SEED </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h4 id="模型多折交叉训练"><a href="#模型多折交叉训练" class="headerlink" title="模型多折交叉训练"></a><strong>模型多折交叉训练</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">skf = KFold(n_splits=FOLDS)</span><br><span class="line"> <span class="keyword">for</span> fold,(train_idx, valid_idx) <span class="keyword">in</span> <span class="built_in">enumerate</span>(skf.split(</span><br><span class="line">             train, train.target )):</span><br><span class="line">         </span><br><span class="line">     <span class="comment"># TRAIN, VALID, TEST FOR FOLD K</span></span><br><span class="line">     Xy_train = IterLoadForDMatrix(train.loc[train_idx], FEATURES, <span class="string">&#x27;target&#x27;</span>)</span><br><span class="line">     X_valid = train.loc[valid_idx, FEATURES]</span><br><span class="line">     y_valid = train.loc[valid_idx, <span class="string">&#x27;target&#x27;</span>]</span><br><span class="line">     </span><br><span class="line">     dtrain = xgb.DeviceQuantileDMatrix(Xy_train, max_bin=<span class="number">256</span>)</span><br><span class="line">     dvalid = xgb.DMatrix(data=X_valid, label=y_valid)</span><br><span class="line">     </span><br><span class="line">     <span class="comment"># TRAIN MODEL FOLD K</span></span><br><span class="line">     model = xgb.train(xgb_parms, </span><br><span class="line">                 dtrain=dtrain,</span><br><span class="line">                 evals=[(dtrain,<span class="string">&#x27;train&#x27;</span>),(dvalid,<span class="string">&#x27;valid&#x27;</span>)],</span><br><span class="line">                 num_boost_round=<span class="number">9999</span>,</span><br><span class="line">                 early_stopping_rounds=<span class="number">100</span>,</span><br><span class="line">                 verbose_eval=<span class="number">100</span>) </span><br><span class="line">     model.save_model(<span class="string">f&#x27;XGB_v<span class="subst">&#123;VER&#125;</span>_fold<span class="subst">&#123;fold&#125;</span>.xgb&#x27;</span>)</span><br><span class="line">     </span><br><span class="line">     <span class="comment"># GET FEATURE IMPORTANCE FOR FOLD K</span></span><br><span class="line">     dd = model.get_score(importance_type=<span class="string">&#x27;weight&#x27;</span>)</span><br><span class="line">     df = pd.DataFrame(&#123;<span class="string">&#x27;feature&#x27;</span>:dd.keys(),<span class="string">f&#x27;importance_<span class="subst">&#123;fold&#125;</span>&#x27;</span>:dd.values()&#125;)</span><br><span class="line">     importances.append(df)</span><br><span class="line">             </span><br><span class="line">     <span class="comment"># INFER OOF FOLD K</span></span><br><span class="line">     oof_preds = model.predict(dvalid)</span><br><span class="line">     acc = amex_metric_mod(y_valid.values, oof_preds)</span><br><span class="line">     <span class="built_in">print</span>(<span class="string">&#x27;Kaggle Metric =&#x27;</span>,acc,<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">     </span><br><span class="line">     <span class="comment"># SAVE OOF</span></span><br><span class="line">     df = train.loc[valid_idx, [<span class="string">&#x27;customer_ID&#x27;</span>,<span class="string">&#x27;target&#x27;</span>] ].copy()</span><br><span class="line">     df[<span class="string">&#x27;oof_pred&#x27;</span>] = oof_preds</span><br><span class="line">     oof.append( df )</span><br><span class="line">     </span><br><span class="line">     <span class="keyword">del</span> dtrain, Xy_train, dd, df</span><br><span class="line">     <span class="keyword">del</span> X_valid, y_valid, dvalid, model</span><br><span class="line">     _ = gc.collect()</span><br></pre></td></tr></table></figure>

<h4 id="深度学习模型"><a href="#深度学习模型" class="headerlink" title="深度学习模型"></a><strong>深度学习模型</strong></h4><p>由于本次比赛数据量比较多，因此深度学习模型也可以考虑。深度学习模型主要需要调整网络结构，下面是一个基础的LSTM代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_model1</span>(<span class="params">n_inputs</span>):</span></span><br><span class="line">    </span><br><span class="line">    x_input = Input(shape=(n_inputs,))</span><br><span class="line">    </span><br><span class="line">    x1 = Bidirectional(LSTM(units=<span class="number">768</span>, return_sequences=<span class="literal">True</span>))(x_input)</span><br><span class="line">    x2 = Bidirectional(LSTM(units=<span class="number">512</span>, return_sequences=<span class="literal">True</span>))(x1)</span><br><span class="line">    x3 = Bidirectional(LSTM(units=<span class="number">384</span>, return_sequences=<span class="literal">True</span>))(x2)</span><br><span class="line">    x4 = Bidirectional(LSTM(units=<span class="number">256</span>, return_sequences=<span class="literal">True</span>))(x3)</span><br><span class="line">    x5 = Bidirectional(LSTM(units=<span class="number">128</span>, return_sequences=<span class="literal">True</span>))(x4)</span><br><span class="line">    </span><br><span class="line">    z2 = Bidirectional(GRU(units=<span class="number">384</span>, return_sequences=<span class="literal">True</span>))(x2)</span><br><span class="line">    </span><br><span class="line">    z31 = Multiply()([x3, z2])</span><br><span class="line">    z31 = BatchNormalization()(z31)</span><br><span class="line">    z3 = Bidirectional(GRU(units=<span class="number">256</span>, return_sequences=<span class="literal">True</span>))(z31)</span><br><span class="line">    </span><br><span class="line">    z41 = Multiply()([x4, z3])</span><br><span class="line">    z41 = BatchNormalization()(z41)</span><br><span class="line">    z4 = Bidirectional(GRU(units=<span class="number">128</span>, return_sequences=<span class="literal">True</span>))(z41)</span><br><span class="line">    </span><br><span class="line">    z51 = Multiply()([x5, z4])</span><br><span class="line">    z51 = BatchNormalization()(z51)</span><br><span class="line">    z5 = Bidirectional(GRU(units=<span class="number">64</span>, return_sequences=<span class="literal">True</span>))(z51)</span><br><span class="line">    </span><br><span class="line">    x = Concatenate(axis=<span class="number">2</span>)([x5, z2, z3, z4, z5])</span><br><span class="line">    </span><br><span class="line">    x = Dense(units=<span class="number">128</span>, activation=<span class="string">&#x27;selu&#x27;</span>)(x)</span><br><span class="line">    </span><br><span class="line">    x_output = Dense(units=<span class="number">1</span>)(x)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=x_input, outputs=x_output, </span><br><span class="line">                  name=<span class="string">&#x27;DNN_Model&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> model            </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><h4 id="meta-features（最重要的特征！）"><a href="#meta-features（最重要的特征！）" class="headerlink" title="meta features（最重要的特征！）"></a>meta features（最重要的特征！）</h4><ul>
<li><p>怎么做</p>
<ol>
<li>train_labels 分配给训练数据（在 cid 聚合之前）和训练模型。</li>
<li>对训练数据进行 oof 预测。</li>
<li>按时间段聚合 oof 预测。</li>
</ol>
</li>
<li><p>参考 DSB2019 的第 2 位解决方法。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/data-science-bowl-2019/discussion/127388">https://www.kaggle.com/c/data-science-bowl-2019/discussion/127388</a></li>
</ul>
</li>
<li><p>为什么<strong>meta features</strong>很重要</p>
<ul>
<li><p>在大多数公共笔记本中，特征都是基于 agg 方法的，例如 min、max、mean 和 std。</p>
</li>
<li><p>尽管这些 agg 特征具有表现力，但在 13 条语句被压缩为有限的统计数据期间，一些信息正在丢失。</p>
</li>
<li><p>我只是想<strong>如何利用每个语句的信息而不是聚合它们。</strong></p>
</li>
<li><p>将 [N_USER, 13, 188] 中的数据扁平化为 [N_USER, 13*188] 是直截了当的方法，但我觉得它会增加太多的特征暗淡。</p>
</li>
<li><p>然后我想出了预测每条语句的默认分数的想法，<strong>可以看作是每条语句的升华</strong>。</p>
</li>
<li><p>这 13 个预测分数就是所谓的<strong>meta features</strong>，在模型中用作每个客户的数字特征。</p>
</li>
</ul>
</li>
</ul>
<h4 id="对于没有meta-features的模型"><a href="#对于没有meta-features的模型" class="headerlink" title="对于没有meta features的模型"></a>对于没有meta features的模型</h4><ul>
<li><p>基本功能来自公共笔记本</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/code/thedevastator/amex-features-the-best-of-both-worlds">https://www.kaggle.com/code/thedevastator/amex-features-the-best-of-both-worlds</a></li>
</ul>
</li>
<li><p>我们对分类（nunique，count，first，mean）进行了通常的聚合<strong>std，mean，min，max</strong>。我们还尝试了 MAD（<strong>Mean Absolute deviation</strong>），它确实对我们的 cv 有所帮助，但是Public LB 得分会稍微下降，所以我们没有在最终private提交中中包含基于 mad 的模型，尽管事后看来它可能会有所帮助。</p>
</li>
<li><p>还添加了百分比变化功能，表示数字特征随时间的百分比变化，在一些模型的尝试中显示比仅使用<strong>numeric diff</strong>效果更好。</p>
</li>
<li><p>在我们的大多数优秀模型中，还有一个特性是保留 <strong>last, first</strong> 和 <strong>middle</strong>，正如我们假设的那样，这将有助于我们覆盖客户的大多数变化，因为我们已经聚合了特征。</p>
</li>
<li><p>我们尝试的另一个功能是计算<strong>spend/balance</strong>或<strong>spend_sum/balance_sum</strong>比率，它在某些模型中确实有帮助，但不是都有效。</p>
</li>
</ul>
<h4 id="混合方法（meta-aggs）："><a href="#混合方法（meta-aggs）：" class="headerlink" title="混合方法（meta+aggs）："></a>混合方法（meta+aggs）：</h4><p>我们还尝试了将扁平化的<strong>meta features</strong>与 <strong>aggregates</strong> 混合在一起，这确实有助于我们的模型，特别是LGBM CAT XGB和Tabnet。<br>由于<strong>meta features</strong>的存在，这种方法通常收敛得更快（在XGB中<strong>earlystop</strong>的轮数更少,其他特征学习的不充分。所以我们不得不为这种方法降低LR，让模型学习慢一点。</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>所有模型都使用<strong>meta features</strong>和<strong>engineered aggregates</strong>进行训练。NN 模型大部分都用 <strong>GaussianScalar</strong> 进行了缩放</p>
<p><strong>LGBM (Max cv 0.79932 Private 0.80731) ：与</strong>基于<a target="_blank" rel="noopener" href="https://www.kaggle.com/ragnar123">@ragnar123</a>开源的Notebook的大多数情况一样。我们确实对超参数做了一些调整，但大多数都是相似的。正如讨论中所述，降低 LR 在 DART 的情况下当然有帮助，而且我们还添加了meta features（大约<em>0.0075 LR</em>）。<br><strong>XGB (Max CV 0.7984 Priv. 0.80687)</strong> : XGB 基于<a target="_blank" rel="noopener" href="https://www.kaggle.com/jiweiliu">@jiweiliu</a> 开源的Notebook<br><strong>CAT (Max CV 0.7972)</strong> : Cat 我们用下面的参数做了一些调整，它帮助了我们很多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CatBoostClassifier( random_state=CFG.seed,  </span><br><span class="line">                                bootstrap_type=<span class="string">&#x27;Bernoulli&#x27;</span>,</span><br><span class="line">                                task_type=<span class="string">&quot;GPU&quot;</span>,</span><br><span class="line">                                devices=<span class="string">&#x27;0:1&#x27;</span>,  </span><br><span class="line">                                use_best_model = <span class="literal">True</span>, </span><br><span class="line">                                iterations = <span class="number">11500</span>,</span><br><span class="line">                                num_leaves =<span class="number">64</span>,</span><br><span class="line">                                subsample = <span class="number">0.74</span>,</span><br><span class="line">                                grow_policy = <span class="string">&#x27;Lossguide&#x27;</span>, </span><br><span class="line">                                depth = <span class="number">9</span>) </span><br></pre></td></tr></table></figure>

<p>**Tabnet (MAX CV 0.793133 Private 0.80447)**：Tabnet 主要使用标准参数，并且在meta features方面确实提供了更好的结果。<br>**MLP（Private 0.80109）:**我们尝试了使用 cat 嵌入和不使用 tabnet 的模型（通过onehotencoding）:<strong>GaussianScaler</strong>技巧有助于 NN模型 和 tabnet模型结果<br>**Tabtransformer (MAX CV 0.79507 Private 0.80480)**：这主要基于公共<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/gauravbrills/tabtransformer-training">notebook</a>和<a target="_blank" rel="noopener" href="https://paperswithcode.com/method/tabtransformer">tabtransformer</a>论文。我们注意到这比private LB 中的 tabnet 更好，但我们没有选择它，因为它在融模中效果不佳😑。下面是超参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NUM_TRANSFORMER_BLOCKS = <span class="number">6</span>  <span class="comment"># Number of transformer blocks. 6 paper recommends</span></span><br><span class="line">NUM_HEADS = <span class="number">8</span>  <span class="comment"># Number of attention heads. 8 Heads paper recommends</span></span><br><span class="line">EMBEDDING_DIMS = <span class="number">16</span><span class="comment">#10  # Embedding dimensions of the categorical features. check 16,32</span></span><br><span class="line">DROPOUT_RATE = <span class="number">0.1</span></span><br><span class="line">MLP_HIDDEN_UNITS_FACTORS = [</span><br><span class="line">    <span class="number">4</span>,</span><br><span class="line">    <span class="number">2</span>,</span><br><span class="line">]  <span class="comment"># MLP hidden layer units, as factors of the number of inputs. =&gt;(4,2)</span></span><br><span class="line">NUM_MLP_BLOCKS = <span class="number">4</span>  <span class="comment"># Number of MLP blocks in the baseline model. Paper 4</span></span><br><span class="line">MLP_ACTIVATION = keras.activations.selu</span><br></pre></td></tr></table></figure>



<h2 id="模型融合技巧"><a href="#模型融合技巧" class="headerlink" title="模型融合技巧"></a>模型融合技巧</h2><p>所有模型融合都是按log融合完成的，因为我们发现它在集成 NN 模型时效果更好。我们也尝试使用 ranking方法进行了集成，但是在使用 NN 集成时效果不佳。</p>
<p>我们尝试了一堆meta集成技术，有很多模型。有效的是Optuna weights 和 Elasticnet 在 oof preds 之上的<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/cdeotte/forward-selection-oof-ensemble-0-942-private">forward selection</a>，我们也尝试了Stacking和Voting技术进行模型融合，但效果都不好。</p>
<h2 id="什么不起作用或没有用？"><a href="#什么不起作用或没有用？" class="headerlink" title="什么不起作用或没有用？"></a>什么不起作用或没有用？</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/code/gauravbrills/tabtransformer-training">Tabformer</a>：我们最终使 tabformer 达到 0.795 cv，但在最终私榜提交中并没有使用该模型。我认为该模型表现优异，但在模型融合中效果不佳，尽管后来我们发现这是我们在 private LB 得分最高的NN模型 。</li>
<li>TCN wavenet：我们尝试在 tcn wavenet + MLP 实现中使用 13 个客户历史序列，但结果只有 0.78 左右，所以我们放弃了这个想法。</li>
<li>Saint 和widedeep 模型：我们还尝试了wide deep package 以尝试 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.01342">SAINT</a> ，但未能成功。参考<a target="_blank" rel="noopener" href="https://github.com/jrzaurin/pytorch-widedeep">pytorch-widedeep</a></li>
<li>如上所述的一些特征和一些基于排列或零特征重要性而被丢弃的特征。</li>
<li>我们尝试为我们所有的有用模型做多种子集成。与坚持使用 seed42相比，这在某种程度上并没有给我们带来好的结果。</li>
<li>最后也尝试了 Target 编码，但我们几乎没有时间在 ensemble 中检查这些是否有效。</li>
</ul>
<p>====================================================================================================================================================================================================================================================================================</p>
<p>如何运行代码</p>
<ol>
<li><p><strong>Install package</strong></p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">pip</span> install optuna eli<span class="number">5</span></span><br><span class="line"><span class="attribute">pip</span> install lightgbm==<span class="number">3</span>.<span class="number">3</span>.<span class="number">2</span></span><br><span class="line"><span class="attribute">pip</span> install xgboost==<span class="number">1</span>.<span class="number">6</span>.<span class="number">1</span></span><br><span class="line"><span class="attribute">pip</span> install modin</span><br><span class="line"><span class="attribute">pip</span> install catboost </span><br><span class="line"></span><br><span class="line"><span class="comment">##cudf安装可根据实际环境安装</span></span><br><span class="line"><span class="comment">#https://rapids.ai/start.html#get-rapids</span></span><br><span class="line"><span class="attribute">conda</span> create -n rapids-<span class="number">22</span>.<span class="number">08</span> -c rapidsai -c nvidia -c conda-forge  \</span><br><span class="line">    <span class="attribute">cudf</span>=<span class="number">22</span>.<span class="number">08</span> python=<span class="number">3</span>.<span class="number">9</span> cudatoolkit=<span class="number">11</span>.<span class="number">2</span></span><br></pre></td></tr></table></figure>

<p><strong>暂时不开源</strong></p>
</li>
<li><p><strong>download data</strong></p>
</li>
</ol>
<p>​        由于本次数据集较大，建议使用kaggle api下载本次比赛数据。(kaggle api使用<a target="_blank" rel="noopener" href="https://www.kaggle.com/docs/api">教程</a>)</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># https://www.kaggle.com/competitions/amex-<span class="keyword">default</span>-prediction/<span class="keyword">data</span></span><br><span class="line">kaggle competitions download -c amex-<span class="keyword">default</span>-prediction</span><br><span class="line"></span><br><span class="line"># https://www.kaggle.com/competitions/amex-<span class="keyword">default</span>-prediction/discussion/<span class="number">328514</span></span><br><span class="line">kaggle datasets download -d raddar/amex-<span class="keyword">data</span>-<span class="keyword">integer</span>-dtypes-parquet-<span class="keyword">format</span></span><br></pre></td></tr></table></figure>



<p><strong>如需尝试该比赛数据，需要内存不低于40G的机器</strong></p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>
        <div id="lv-container"></div>
        <div class="giscus"></div>
    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        <li>
            <a target="_blank" href="https://twitter.com/iconie_alloy">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-twitter"></i>
                            </span>
            </a>
        </li>
        
        
        <li>
            <a target="_blank" href="https://www.zhihu.com/people/ai-er-lan-xue-da">
                            <span class="fa-stack fa-lg">
                                 <i class="iconfont icon-zhihu"></i>
                            </span>
            </a>
        </li>
        

        
        <li>
            <a target="_blank" href="http://weibo.com/3286578617">
                            <span class="fa-stack fa-lg">
                                  <i class="iconfont icon-weibo"></i>
                            </span>
            </a>
        </li>
        

        
        <li>
            <a target="_blank" href="https://www.facebook.com/xiaotao.nie.5">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-facebook"></i>
                            </span>
            </a>
        </li>
        

        
        <li>
            <a target="_blank"  href="https://github.com/albert51966">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-github"></i>
                            </span>
            </a>
        </li>
        

        
        <li>
            <a target="_blank"  href="https://www.linkedin.com/in/小涛-聂-80964aba">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-linkedin"></i>
                            </span>
            </a>
        </li>
        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a target="_blank" rel="noopener" href="https://www.10000h.top">10000H</a></span>
        <span>/</span>
        
        <span><a href="https://weihaocai.top">Albert&#39;s Page</a></span>
        <span>/</span>
        
        <span><a href="#">It helps SEO</a></span>
        <span>/</span>
        
        <span><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/">浙ICP备16035324号-1</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>  Theme <a target="_blank" rel="noopener" href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>

<script src="/js/index.js"></script>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




    <script src="https://giscus.app/client.js"
    data-repo="aircloud/hexo-aircloud-blog"
    data-repo-id="MDEwOlJlcG9zaXRvcnkxMjkwNDgyNjg="
    data-category="Announcements"
    data-category-id="DIC_kwDOB7EezM4COhKJ"
    data-mapping="title"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme="light"
    data-lang="zh-CN"
    crossorigin="anonymous"
    async>
</script>




</html>
